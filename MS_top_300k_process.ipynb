{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from fuzzywuzzy import fuzz\n",
    "from seal import FMIndex, SEALSearcher\n",
    "from transformers import AutoTokenizer\n",
    "from seal.evaluate import evaluator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docmap = {}\n",
    "\n",
    "with open(\"../../../data/msmarco-data/msmarco-docs-sents.top.300k.json\", \"r\") as fin:\n",
    "    for i, line in enumerate(tqdm(fin)):\n",
    "        line = json.loads(line)\n",
    "        docid, body, title, url = line['docid'].lower(), line['body'], line['title'], line['url']\n",
    "        body = re.sub('<[^<]+?>', '', body).replace('\\n', '').strip()\n",
    "        body = re.sub(' +', ' ', body)\n",
    "        body = body.split()[:128]\n",
    "        body = \" \".join(body)\n",
    "        docmap[docid] = {'docid' : docid, 'url' : url, 'title' : title, 'body' : body}\n",
    "    \n",
    "with open(\"MS_top300k/corpus.json\", \"w\") as fout:\n",
    "    for k, v in docmap.items():\n",
    "        line = json.dumps(v)\n",
    "        fout.write(line + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Training/Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docmap = {}\n",
    "with open(\"MS_top300k/corpus.json\", \"r\") as fin:\n",
    "    for i, line in enumerate(tqdm(fin)):\n",
    "        line = json.loads(line)\n",
    "        docmap[line['docid']] = line\n",
    "        \n",
    "print(\"load corpus succees\")\n",
    "\n",
    "# dev\n",
    "dev_querymap = {}\n",
    "with open('../NCI/Data_process/MS_dataset/top_300k/queries.dev.tsv','r') as f:\n",
    "    for line in tqdm(enumerate(f)):\n",
    "        line = line[1].split(\"\\t\")\n",
    "        dev_querymap[line[0]] = line[1].strip()\n",
    "\n",
    "devset = []\n",
    "dev = []\n",
    "with open(\"../NCI/Data_process/MS_dataset/top_300k/qrels.dev.tsv\", \"r\") as f:\n",
    "    for line in tqdm(enumerate(f)):\n",
    "        qid, _, docid, _ = line[1].strip().split(\" \")\n",
    "        query = dev_querymap[qid]\n",
    "        docid = docid.lower() \n",
    "        devset.append({'qid' : qid, 'docid' : docid , 'query' : query})\n",
    "        dev.append({'query' : query, 'docid' : docid, 'title' : docmap[docid]['title'], 'body' : docmap[docid]['body']})\n",
    "        \n",
    "with open(\"MS_top300k/dev4retrieval.json\", \"w\") as f:\n",
    "    for it in devset:\n",
    "        line = json.dumps(it)\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(\"MS_top300k/dev.json\", \"w\") as f:\n",
    "    for it in tqdm(dev):\n",
    "        line = json.dumps(it)\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"save dev set succees\")\n",
    "\n",
    "#train \n",
    "train_querymap = {}\n",
    "with open('../NCI/Data_process/MS_dataset/top_300k/queries.train.tsv','r') as f:\n",
    "    for line in tqdm(enumerate(f)):\n",
    "        line = line[1].split(\"\\t\")\n",
    "        train_querymap[line[0]] = line[1].strip()\n",
    "\n",
    "train = []\n",
    "with open(\"../NCI/Data_process/MS_dataset/top_300k/qrels.train.tsv\", \"r\") as f:\n",
    "    for line in tqdm(enumerate(f)):\n",
    "        qid, _, docid, _ = line[1].strip().split(\" \")\n",
    "        docid = docid.lower() \n",
    "        query = train_querymap[qid]\n",
    "        train.append({'query' : query, 'docid' : docid, 'title' : docmap[docid]['title'], 'body' : docmap[docid]['body']})\n",
    "\n",
    "with open(\"MS_top300k/train.json\", \"w\") as f:\n",
    "    for it in tqdm(train):\n",
    "        line = json.dumps(it)\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"save train set succees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = 50\n",
    "n_samples = 10\n",
    "min_length = 10\n",
    "max_length = 10\n",
    "temperature = 1.0\n",
    "banned = set(stopwords.words('english'))\n",
    "\n",
    "def iterator_title(train_or_dev):\n",
    "    with open(f\"MS_top300k/{train_or_dev}.json\", \"r\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            sample = json.loads(line)\n",
    "            yield sample['query'].strip() + \" || title || +\", sample['title'].strip() + \" @@\"\n",
    "\n",
    "def _iterator_span_get_arguments(train_or_dev):\n",
    "    with open(f\"MS_top300k/{train_or_dev}.json\", \"r\") as fin:\n",
    "        for line in tqdm(fin):\n",
    "            sample = json.loads(line)\n",
    "            yield sample['body'].strip(), sample['query'].strip() + \" || body || +\"\n",
    "\n",
    "def span_iterator(tokens, ngrams=3, banned=banned):\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in banned:\n",
    "            yield (i, i+ngrams)\n",
    "\n",
    "def extract_spans(text, source, n_samples, min_length, max_length, temperature=1.0):\n",
    "    source = source.split(\"||\", 1)[0]\n",
    "    query_tokens = source.split()\n",
    "    query_tokens_lower = [t.lower() for t in query_tokens]\n",
    "    passage_tokens = text.split()\n",
    "    passage_tokens_lower = [t.lower() for t in passage_tokens]\n",
    "    matches = defaultdict(int)\n",
    "    for i1, _ in enumerate(query_tokens_lower):\n",
    "        j1 = i1+3\n",
    "        str_1 = \" \".join(query_tokens_lower[i1:j1])\n",
    "        for (i2, j2) in span_iterator(passage_tokens_lower, 3):\n",
    "            str_2 = \" \".join(passage_tokens_lower[i2:j2])\n",
    "            ratio = fuzz.ratio(str_1, str_2) / 100.0\n",
    "            matches[i2] += ratio\n",
    "\n",
    "    if not matches:\n",
    "        indices = [0]\n",
    "    else:\n",
    "        indices, weights = zip(*sorted(matches.items(), key=lambda x: -(x[1])))\n",
    "        weights = list(weights)\n",
    "        sum_weights = float(sum([0] + weights))\n",
    "        if sum_weights == 0.0 or not weights:\n",
    "            indices = [0]\n",
    "            weights = [1.0]\n",
    "        else:\n",
    "            weights = [math.exp(float(w) / temperature) for w in weights]\n",
    "            Z = sum(weights)\n",
    "            weights = [w / Z for w in weights]\n",
    "        indices = random.choices(indices, weights=weights, k=n_samples)\n",
    "\n",
    "    for i in indices:\n",
    "        subspan_size = random.randint(min_length, max_length)\n",
    "        span = \" \".join(passage_tokens[i:i+subspan_size])\n",
    "        yield span\n",
    "\n",
    "def extract_spans_wrapper(args):\n",
    "    return args[1], list(extract_spans(*args))\n",
    "\n",
    "def iterator_span(train_or_dev):\n",
    "    arg_it = _iterator_span_get_arguments(train_or_dev)\n",
    "    arg_it = ((text, source, n_samples, min_length, max_length, temperature) for text, source in arg_it)\n",
    "    with multiprocessing.Pool(jobs) as pool:\n",
    "        for source, spans in pool.imap(extract_spans_wrapper, arg_it):\n",
    "            for target in spans:\n",
    "                yield source, target    \n",
    "\n",
    "def gen_title_ex(train_or_dev):\n",
    "    with open(f\"MS_top300k/{train_or_dev}.source\", 'w') as src, open(f\"MS_top300k/{train_or_dev}.target\", 'w') as tgt:\n",
    "        for source, target in iterator_title(train_or_dev):\n",
    "            src.write(\" \" + source.strip() + \"\\n\")\n",
    "            tgt.write(\" \" + target.strip() + \"\\n\")\n",
    "        \n",
    "def get_span_ex(train_or_dev):\n",
    "    with open(f\"MS_top300k/{train_or_dev}.source\", 'a') as src, open(f\"MS_top300k/{train_or_dev}.target\", 'a') as tgt:\n",
    "        for source, target in iterator_span(train_or_dev):\n",
    "            src.write(\" \" + source.strip() + \"\\n\")\n",
    "            tgt.write(\" \" + target.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_title_ex(\"dev\")\n",
    "get_span_ex(\"dev\")\n",
    "gen_title_ex(\"train\")\n",
    "get_span_ex(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned = {\n",
    "    \"the\", \"The\",\n",
    "    \"to\", \n",
    "    \"a\", \"A\", \"an\", \"An\", \n",
    "    \"he\", \"He\", \"his\", \"His\", \"him\", \"He's\",  \n",
    "    \"she\", \"She\", \"her\", \"Her\", \"she's\", \"She's\", \n",
    "    \"it\", \"It\", \"its\", \"Its\",  \"it's\", \"It's\",\n",
    "    \"and\", \"And\",\n",
    "    \"or\", \"Or\",\n",
    "    \"this\", \"This\",\n",
    "    \"that\", \"That\",\n",
    "    \"those\", \"Those\",\n",
    "    \"these\", \"These\",\n",
    "    '\"', '\"\"', \"'\", \"''\",\n",
    "}\n",
    "\n",
    "def is_good(token):\n",
    "    if token in banned:\n",
    "        return False\n",
    "    elif token[-1] in '?.!':\n",
    "        return False\n",
    "    elif token[0] in '([':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def preprocess_file(\n",
    "    input_path,\n",
    "    num_samples=1,\n",
    "    num_title_samples=1,\n",
    "    delimiter='@@', \n",
    "    min_length_input=1,\n",
    "    max_length_input=15,\n",
    "    min_length_output=10, \n",
    "    max_length_output=10,\n",
    "    full_doc_n=0,\n",
    "    ):\n",
    "    \n",
    "    with open(input_path, 'r', 2 ** 20) as f:\n",
    "        for line in tqdm(f):\n",
    "            line=json.loads(line)\n",
    "            text = line['body']\n",
    "            title = line['title']\n",
    "\n",
    "            if text == title:\n",
    "                continue\n",
    "\n",
    "            tokens = text.split()\n",
    "\n",
    "            for _ in range(full_doc_n):\n",
    "                a = text.strip() + \" || title || p\"\n",
    "                b = title.strip() + \" \" + delimiter\n",
    "                yield a, b\n",
    "\n",
    "            sampled = 0\n",
    "            failures = 0\n",
    "            while sampled < num_title_samples and failures < 10:\n",
    "\n",
    "                if random.random() > 0.5:\n",
    "                    len_a = random.randint(min_length_input, max_length_input)\n",
    "                    idx_a = random.randint(0, max(0, len(tokens)-len_a))\n",
    "                    a = ' '.join(tokens[idx_a:idx_a+len_a]).strip() + \" || title || p\"\n",
    "                    b = title.strip() + \" \" + delimiter\n",
    "                    \n",
    "                else:\n",
    "\n",
    "                    len_b = random.randint(min_length_output, max_length_output)\n",
    "                    idx_b = random.randint(0, max(0, len(tokens)-len_b))\n",
    "                    \n",
    "                    if idx_b >= len(tokens):\n",
    "                        failures += 1\n",
    "                        continue\n",
    "                    \n",
    "                    if not is_good(tokens[idx_b]):\n",
    "                        failures += 1\n",
    "                        continue\n",
    "\n",
    "                    b = ' '.join(tokens[idx_b:idx_b+len_b]).strip()\n",
    "                    a = title.strip() + ' || body || p'\n",
    "                    \n",
    "                yield a, b\n",
    "                sampled += 1\n",
    "\n",
    "            sampled = 0\n",
    "            failures = 0\n",
    "            while sampled < num_samples and failures < 10:\n",
    "                len_a = random.randint(min_length_input, max_length_input)\n",
    "                len_b = random.randint(min_length_output, max_length_output)\n",
    "                idx_a = random.randint(0, max(0, len(tokens)-len_a))\n",
    "                idx_b = random.randint(0, max(0, len(tokens)-len_b))\n",
    "\n",
    "                if idx_a == idx_b or (not is_good(tokens[idx_b])):\n",
    "                    failures += 1\n",
    "                    continue\n",
    "\n",
    "                a = ' '.join(tokens[idx_a:idx_a+len_a]).strip() + ' || body || p'\n",
    "                b = ' '.join(tokens[idx_b:idx_b+len_b]).strip()\n",
    "                yield a, b\n",
    "                sampled += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"MS_top300k/unsupervised.source\", 'w', 2 ** 20) as src, open(\"MS_top300k/unsupervised.target\", 'w', 2 ** 20) as tgt:\n",
    "    for i, (s, t) in enumerate(preprocess_file(\n",
    "        \"MS_top300k/train.json\",\n",
    "        num_samples=3,\n",
    "        num_title_samples=1,\n",
    "        full_doc_n=1,\n",
    "        delimiter=\"@@\",\n",
    "        min_length_input=10,\n",
    "        max_length_input=10,\n",
    "        min_length_output=10,\n",
    "        max_length_output=10,       \n",
    "    )):\n",
    "        if random.random() < 0.1:\n",
    "            s = s.lower()\n",
    "        s = \" \" + s\n",
    "        t = \" \" + t\n",
    "        src.write(s + '\\n')\n",
    "        tgt.write(t + '\\n')\n",
    "\n",
    "os.system(\"cat MS_top300k/unsupervised.source >> MS_top300k/train.source\")\n",
    "os.system(\"cat MS_top300k/unsupervised.target >> MS_top300k/train.target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh ./scripts/training/preprocess_fairseq.sh MS_top300k BART"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building FM-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "labels = []\n",
    "\n",
    "with open(\"MS_top300k/corpus.json\", \"r\") as fin:\n",
    "    for i, line in enumerate(tqdm(fin)):\n",
    "        line = json.loads(line)\n",
    "        labels.append(line['docid'])\n",
    "        corpus.append(line['title'] + \" @@ \" + line['body'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "def preprocess(doc):\n",
    "    doc = ' ' + doc\n",
    "    doc = tokenizer(doc, add_special_tokens=False, truncation = True, max_length=1023)['input_ids']\n",
    "    doc += [tokenizer.eos_token_id]\n",
    "    return doc\n",
    "\n",
    "\n",
    "corpus_tokenized = [preprocess(doc) for doc in tqdm(corpus)]\n",
    "\n",
    "with open(\"MS_top300k/tokenized_corpus.json\", \"w\") as fout:\n",
    "    for i, text in tqdm(enumerate(corpus_tokenized)):\n",
    "        it = {'label' : labels[i], 'doc' : text}\n",
    "        it = json.dumps(it)\n",
    "        fout.write(it+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = []\n",
    "labels = []\n",
    "with open(\"MS_top300k/tokenized_corpus.json\", \"r\") as fin:\n",
    "    for line in tqdm(fin):\n",
    "        line = json.loads(line)\n",
    "        labels.append(line['label'])\n",
    "        corpus_tokenized.append(line['doc'])\n",
    "        \n",
    "\n",
    "index = FMIndex()\n",
    "index.initialize(corpus_tokenized, in_memory=True)\n",
    "index.labels = labels\n",
    "index.save('MS_top300k/FM_Index/MS_top300k.fm_index')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sh ./scripts/training/training_fairseq.sh MS_top300k BART"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = SEALSearcher.load(\"MS_top300k/FM_Index/MS_top300k.fm_index\", \"checkpoints/checkpoint_best.pt\", device='cuda:0' )\n",
    "searcher.include_keys = True\n",
    "myevaluator = evaluator()\n",
    "\n",
    "query_list = []\n",
    "result = []\n",
    "truth = []\n",
    "\n",
    "with open(\"MS_top300k/dev4retrieval.json\", \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        line = json.loads(line)\n",
    "        tmp = []\n",
    "        truth.append([line['docid']])\n",
    "        for doc in searcher.search(line['query'], k=10):\n",
    "            tmp.append(doc.docid)\n",
    "        result.append(tmp)\n",
    "\n",
    "res = myevaluator.evaluate_ranking(truth, result)\n",
    "print(f\"mrr@5:{res['mrr5']}, mrr@10:{res['mrr10']}, mrr:{res['mrr']}, p@1:{res['p1']}, p@10:{res['p10']}, p@20:{res['p20']}, p@100:{res['p100']}, r@1:{res['r1']}, r@5:{res['r5']}, r@10:{res['r10']}, r@100:{res['r100']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ce5b21adb3ad27f9ea0a4d71aa9a1c6d1e3bdf5b4bd46ac2b3bc99e86072cce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
