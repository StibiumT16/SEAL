{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the FM-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19761, 112, 49314, 152, 16, 10, 7728, 3780, 2], [19761, 132, 49314, 152, 16, 277, 7728, 3780, 2], [19761, 155, 49314, 178, 259, 47, 465, 5, 507, 65, 2]]\n",
      "[0, 9, 18, 29]\n",
      "[65, 2, 259, 3780, 132, 5, 10, 16, 465, 277, 152, 155, 49314, 47, 112, 19761, 7728, 178, 507]\n",
      "[2, 5, 10, 16, 47, 65, 112, 132, 152, 155, 259, 277, 465, 507, 3780, 7728, 19761, 49314]\n",
      "[3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 3, 3]\n",
      "['doc1', 'doc2', 'doc3']\n"
     ]
    }
   ],
   "source": [
    "from seal import FMIndex\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "corpus = [\n",
    "    \"Doc 1 @@ This is a sample document\",\n",
    "    \"Doc 2 @@ This is another sample document\",\n",
    "    \"Doc 3 @@ And here you find the final one\",\n",
    "]\n",
    "labels = ['doc1', 'doc2', 'doc3']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "def preprocess(doc):\n",
    "    doc = ' ' + doc\n",
    "    doc = tokenizer(doc, add_special_tokens=False)['input_ids']\n",
    "    doc += [tokenizer.eos_token_id]\n",
    "    return doc\n",
    "\n",
    "corpus_tokenized = [preprocess(doc) for doc in corpus]\n",
    "print(corpus_tokenized)\n",
    "\n",
    "index = FMIndex()\n",
    "index.initialize(corpus_tokenized, in_memory=True)\n",
    "index.labels = labels\n",
    "\n",
    "index.save('res/sample/sample_corpus.fm_index')\n",
    "# writes res/sample/sample_corpus.fm_index.fmi\n",
    "# writes res/sample/sample_corpus.fm_index.oth\n",
    "\n",
    "index = FMIndex.load('res/sample/sample_corpus.fm_index')\n",
    "print(index.beginnings)\n",
    "print(index.occurring)\n",
    "print(index.occurring_distinct)\n",
    "print(index.occurring_counts)\n",
    "print(index.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with the FM-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (170 > 60). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unicorns welcomed the researchers and explained that they had been waiting for them for a very long time.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from seal import fm_index_generate, FMIndex\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('pegasus_paraphrase')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('pegasus_paraphrase')\n",
    "\n",
    "# building the corpus from a single long string\n",
    "corpus = \" \".join(\"\"\"\n",
    "They also were found to have perfectly coiffed hair, and wore what appeared to be Dior makeup. \n",
    "“We were shocked to discover the unicorns,” said anthropologist Daniel St. Maurice. “They were \n",
    "like nothing we had ever seen before. We had heard legends of the unicorns, but never thought \n",
    "they actually existed.” When the scientists first arrived in the valley, the unicorns were \n",
    "surprised and startled by the presence of humans, but were also excited. The unicorns welcomed \n",
    "the researchers and explained that they had been waiting for them for a very long time. “The \n",
    "unicorns said that they had been waiting for us for a very long time,” said Dr. St. Maurice. \n",
    "“They said they had always known that humans would eventually discover them, but that they had \n",
    "also always known that humans would be too stupid to realize the unicorns had been waiting for \n",
    "them.”\n",
    "\"\"\".split()).strip()\n",
    "corpus = tokenizer(' ' + corpus, add_special_tokens=False)['input_ids'] + [tokenizer.eos_token_id]\n",
    "index = FMIndex()\n",
    "index.initialize([corpus], in_memory=True)\n",
    "\n",
    "# constrained generation\n",
    "query = \" \".join(\"\"\"\n",
    "The unicorns greeted the scientists, explaining that they had been expecting the encounter for\n",
    "a while.'\n",
    "”\"\"\".split()).strip()\n",
    "out = fm_index_generate(\n",
    "    model, index,\n",
    "    **tokenizer([' ' + query], return_tensors='pt'),\n",
    "    keep_history=False,\n",
    "    transformers_output=True,\n",
    "    always_allow_eos=True,\n",
    "    max_length=100,\n",
    ")\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True).strip())\n",
    "# unicorns welcomed the researchers and explained that they had been waiting for them for a very long time.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-14 15:12:21,761 - seal.retrieval - WARNING - initializing FM-index from ./ckpt/NQ/NQ.fm_index\n",
      "2023-01-14 15:12:54,204 - seal.retrieval - WARNING - FM-index initialized (13868 MBs)\n",
      "2023-01-14 15:12:54,205 - seal.retrieval - WARNING - initializing BART large\n",
      "2023-01-14 15:13:29,738 - seal.retrieval - WARNING - loading weights from checkpoint: ./ckpt/NQ/SEAL.NQ.pt\n",
      "2023-01-14 15:13:39,360 - seal.retrieval - WARNING - model successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t510.01721395707966\t6957412\tTable manners\tshould not chew or bite food from the fork. The knife should be held with the base into the palm of the hand, not like a pen with the base resting between the thumb and forefinger. The knife must never enter the mouth or be licked. When eating soup, the spoon is held in the right hand and the bowl tipped away from the diner, scooping the soup in outward movements. The soup spoon should never be put into the mouth, and soup should be sipped from the side of the spoon, not the end. Food should always be chewed\n",
      "Matched:\n",
      "161.3\t10\t' eating soup'\n",
      "126.3\t23\t'</s> Table manners @@'\n",
      "059.5\t9390\t' fork'\n",
      "052.5\t373\t' spoon,'\n",
      "020.3\t236262\t' food'\n",
      "1\t457.43813062425585\t13796077\tChopsticks\tare similar, finer points can differ from region to region. In Cambodia, a fork and spoon are the typical utensils used in Cambodian dining and etiquette. Spoons are used to scoop up food or water and the fork is there to help guide the food onto the spoon. Chopsticks are normally used in noodle dishes such as the Kuy Tiev and soup dishes. When eating soup the chopsticks will typically be paired with the spoon, where the chopsticks will pick up the food and the spoon will be used to drink the broth. Forks are never to touch the mouth,\n",
      "Matched:\n",
      "161.3\t10\t' eating soup'\n",
      "072.9\t210\t' fork is'\n",
      "052.5\t373\t' spoon,'\n",
      "031.8\t17622\t' dishes'\n",
      "020.3\t236262\t' food'\n",
      "2\t439.08149883110417\t6957430\tTable manners\twithout first asking for permission, but, if offered the last bit of food in the communal dish, it is considered rude to refuse the offer. Bowls of rice or soup should not be picked up off the table while dining, an exception being made for large bowls of Korean noodle soup. Slurping while eating noodles and soup is generally acceptable. It is not uncommon to chew with the mouth open. If alcohol is served with the meal, it is common practice that when alcohol is first served for the eldest/highest-ranked diner to make a toast and for diners to clink\n",
      "Matched:\n",
      "126.3\t23\t'</s> Table manners @@'\n",
      "064.6\t10815\t' soup'\n",
      "045.9\t28764\t' eating'\n",
      "033.4\t24084\t' dish'\n",
      "026.1\t18409\t' meal'\n"
     ]
    }
   ],
   "source": [
    "from seal import SEALSearcher\n",
    "\n",
    "searcher = SEALSearcher.load('./ckpt/NQ/NQ.fm_index', './ckpt/NQ/SEAL.NQ.pt')\n",
    "searcher.include_keys = True\n",
    "\n",
    "query = \"can you eat soup with a fork\"\n",
    "\n",
    "for i, doc in enumerate(searcher.search(query, k=3)):\n",
    "    print(i, doc.score, doc.docid, *doc.text(), sep='\\t')\n",
    "    print(\"Matched:\")\n",
    "    matched = sorted(doc.keys, reverse=True, key=lambda x:x[2])\n",
    "    matched = matched[:5]\n",
    "    for ngram, freq, score in matched:\n",
    "        print(\"{:.1f}\".format(score).zfill(5), freq, repr(ngram), sep='\\t')\n",
    "\n",
    "# 0\t375.03041350768547\t13796077\tChopsticks\tare similar, finer points can differ from region to region. \n",
    "# In Cambodia, a fork and spoon are the typical utensils used in Cambodian dining and etiquette. Spoons are \n",
    "# used to scoop up food or water and the fork is there to help guide the food onto the spoon. Chopsticks \n",
    "# are normally used in noodle dishes such as the Kuy Tiev and soup dishes. When eating soup the chopsticks \n",
    "# will typically be paired with the spoon, where the chopsticks will pick up the food and the spoon will be \n",
    "# used to drink the broth. Forks are never to touch the mouth,\n",
    "# Matched:\n",
    "# 161.3\t10\t' eating soup'\n",
    "# 059.5\t9390\t' fork'\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEAL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 13:49:34) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adafd9229683e5efe0f5b6ca46342f48f7190bcb1f880ca595d28228faa64ac8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
